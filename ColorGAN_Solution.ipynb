{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ColorGAN_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shigerufw/ML_Introduction/blob/main/ColorGAN_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVnF24NFcuQ6"
      },
      "source": [
        "## **Mount Google Drive to store files & data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzVT_cB7cZFp",
        "outputId": "011efa76-5c58-4e48-af65-be309f5c95ff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJp-D51g0IDd"
      },
      "source": [
        "## **1) Importing Python Packages for GAN**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k5mFBuzzl2a"
      },
      "source": [
        "# from keras.datasets import cifar10, mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D, Dense, Conv2DTranspose\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "!mkdir generated_images resized_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoRqPt1DwtD_"
      },
      "source": [
        "##!unzip the-zip-file -d name-of-destination-folder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRWx5D09dX3p"
      },
      "source": [
        "images_path = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaoY5WTbi4dJ"
      },
      "source": [
        "## **Resizing Data to match Neural Network Input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUJtmq0gzkSx"
      },
      "source": [
        "\n",
        "import os\n",
        "# from PIL import Image\n",
        "import cv2\n",
        "reshape_size = (64,64)\n",
        "\n",
        "i = 0\n",
        "for image in os.listdir(images_path):\n",
        "  # print(image)\n",
        "  img = cv2.imread(images_path + image)\n",
        "  img = cv2.resize(img, reshape_size)\n",
        "  cv2.imwrite(\"resized_images/%d.png\" % i,img)\n",
        "  # # print(img.shape)\n",
        "  i = i+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr-eZOzg0X79"
      },
      "source": [
        "## **2) Parameters for Neural Networks & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RThZMDruz9cB",
        "outputId": "47c5f84f-244a-49ed-9abd-6bdb98255c01"
      },
      "source": [
        "img_width = 64\n",
        "img_height = 64\n",
        "channels = 3\n",
        "img_shape = (img_width, img_height, channels)\n",
        "latent_dim = 100\n",
        "adam = Adam(lr=0.0002)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bcJZZg0cqy"
      },
      "source": [
        "## **3) Building Generator**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdiqZpri0iQh"
      },
      "source": [
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256 * 8* 8, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((8,8,256)))\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "  \n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "generator = build_generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt6QsJCW0mcI"
      },
      "source": [
        "## **4) Building Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2JzEAPv0lKt"
      },
      "source": [
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3,3), padding='same', input_shape=img_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), padding='same', ))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    model.add(Conv2D(128, (3,3), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2D(256, (3,3), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbcKcKmA0q2S"
      },
      "source": [
        "## **5) Connecting Neural Networks to build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ue3TEd0xLy"
      },
      "source": [
        "GAN = Sequential()\n",
        "discriminator.trainable = False\n",
        "GAN.add(generator)\n",
        "GAN.add(discriminator)\n",
        "\n",
        "GAN.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPqU8dZDaQmE"
      },
      "source": [
        "# generator.summary()\n",
        "# discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WaNhBDwRwTG"
      },
      "source": [
        "## **6) Outputting Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQEJ0WbjRppy"
      },
      "source": [
        "#@title\n",
        "## **7) Outputting Images**\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000000\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    r, c = 4, 4\n",
        "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    global save_name\n",
        "    save_name += 0.00000001\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = (gen_imgs + 1) / 2.0\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"currentgeneration.png\")\n",
        "    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE57Lk5V0xs2"
      },
      "source": [
        "## **7) Training GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "egSJJvik00Iq",
        "outputId": "b6f6f355-53ee-4227-fdeb-63ad41cc642f"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "def train(epochs, batch_size=32, save_interval=200):\n",
        "\n",
        "  array = []\n",
        "  #PUT PATH OF RESIZED IMAGES\n",
        "  path = \"/content/drive/MyDrive/resized_images/\"\n",
        "\n",
        "  for dir in os.listdir(path):\n",
        "            # print(dir)\n",
        "    image = Image.open(path + dir)\n",
        "    data = np.asarray(image)\n",
        "    array.append(data)\n",
        "\n",
        "  X_train = np.array(array)\n",
        "  print(X_train.shape)\n",
        "\n",
        "  # print(X_train.shape)\n",
        "  #Rescale data between -1 and 1\n",
        "  X_train = X_train / 127.5 -1.\n",
        "  bat_per_epo = int(X_train.shape[0] / batch_size)\n",
        "  # X_train = np.expand_dims(X_train, axis=3)\n",
        "  print(X_train.shape)\n",
        "\n",
        "  #Create our Y for our Neural Networks\n",
        "  valid = np.ones((batch_size, 1))\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for j in range(bat_per_epo):\n",
        "      #Get Random Batch\n",
        "      idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "      imgs = X_train[idx]\n",
        "\n",
        "      #Generate Fake Images\n",
        "      noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "      gen_imgs = generator.predict(noise)\n",
        "\n",
        "      #Train discriminator\n",
        "      d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "      d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "      noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "      \n",
        "      #inverse y label\n",
        "      g_loss = GAN.train_on_batch(noise, valid)\n",
        "\n",
        "      print(\"******* %d %d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch,j, d_loss[0], 100* d_loss[1], g_loss))\n",
        "\n",
        "      # if(epoch % save_interval) == 0:\n",
        "    save_imgs(epoch)\n",
        "\n",
        "\n",
        "train(30000, batch_size=32, save_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2612, 64, 64, 3)\n",
            "******* 0 0 [D loss: 0.705788, acc: 18.75%] [G loss: 0.687168]\n",
            "******* 0 1 [D loss: 0.353064, acc: 56.25%] [G loss: 0.721375]\n",
            "******* 0 2 [D loss: 0.327350, acc: 100.00%] [G loss: 0.787092]\n",
            "******* 0 3 [D loss: 0.288428, acc: 100.00%] [G loss: 0.913746]\n",
            "******* 0 4 [D loss: 0.229412, acc: 100.00%] [G loss: 1.150219]\n",
            "******* 0 5 [D loss: 0.163720, acc: 100.00%] [G loss: 1.521333]\n",
            "******* 0 6 [D loss: 0.149518, acc: 96.88%] [G loss: 1.741902]\n",
            "******* 0 7 [D loss: 0.089338, acc: 100.00%] [G loss: 2.085894]\n",
            "******* 0 8 [D loss: 0.057856, acc: 100.00%] [G loss: 2.599167]\n",
            "******* 0 9 [D loss: 0.030943, acc: 100.00%] [G loss: 3.334183]\n",
            "******* 0 10 [D loss: 0.013668, acc: 100.00%] [G loss: 4.292578]\n",
            "******* 0 11 [D loss: 0.005061, acc: 100.00%] [G loss: 5.523189]\n",
            "******* 0 12 [D loss: 0.001345, acc: 100.00%] [G loss: 6.966303]\n",
            "******* 0 13 [D loss: 0.000354, acc: 100.00%] [G loss: 8.759720]\n",
            "******* 0 14 [D loss: 0.000051, acc: 100.00%] [G loss: 10.982534]\n",
            "******* 0 15 [D loss: 0.000005, acc: 100.00%] [G loss: 13.722927]\n",
            "******* 0 16 [D loss: 0.000000, acc: 100.00%] [G loss: 16.284195]\n",
            "******* 0 17 [D loss: 0.000000, acc: 100.00%] [G loss: 17.507738]\n",
            "******* 0 18 [D loss: 0.000000, acc: 100.00%] [G loss: 16.099010]\n",
            "******* 0 19 [D loss: 0.000040, acc: 100.00%] [G loss: 11.689158]\n",
            "******* 0 20 [D loss: 0.069643, acc: 100.00%] [G loss: 29.770477]\n",
            "******* 0 21 [D loss: 0.000000, acc: 100.00%] [G loss: 70.194519]\n",
            "******* 0 22 [D loss: 0.479776, acc: 96.88%] [G loss: 61.224049]\n",
            "******* 0 23 [D loss: 0.174727, acc: 96.88%] [G loss: 32.452599]\n",
            "******* 0 24 [D loss: 0.431338, acc: 96.88%] [G loss: 0.449597]\n",
            "******* 0 25 [D loss: 7.537210, acc: 50.00%] [G loss: 0.228415]\n",
            "******* 0 26 [D loss: 0.100642, acc: 100.00%] [G loss: 10.505826]\n",
            "******* 0 27 [D loss: 0.239416, acc: 96.88%] [G loss: 15.499646]\n",
            "******* 0 28 [D loss: 0.000000, acc: 100.00%] [G loss: 17.391134]\n",
            "******* 0 29 [D loss: 0.545655, acc: 96.88%] [G loss: 15.243996]\n",
            "******* 0 30 [D loss: 0.525732, acc: 96.88%] [G loss: 11.592022]\n",
            "******* 0 31 [D loss: 0.000039, acc: 100.00%] [G loss: 9.271732]\n",
            "******* 0 32 [D loss: 0.000190, acc: 100.00%] [G loss: 7.581604]\n",
            "******* 0 33 [D loss: 0.000949, acc: 100.00%] [G loss: 6.046256]\n",
            "******* 0 34 [D loss: 0.004465, acc: 100.00%] [G loss: 4.853039]\n",
            "******* 0 35 [D loss: 0.012309, acc: 100.00%] [G loss: 3.837401]\n",
            "******* 0 36 [D loss: 0.038452, acc: 100.00%] [G loss: 3.133594]\n",
            "******* 0 37 [D loss: 0.103187, acc: 96.88%] [G loss: 3.738275]\n",
            "******* 0 38 [D loss: 0.360478, acc: 87.50%] [G loss: 4.390351]\n",
            "******* 0 39 [D loss: 0.543657, acc: 84.38%] [G loss: 5.803115]\n",
            "******* 0 40 [D loss: 0.064667, acc: 96.88%] [G loss: 6.896056]\n",
            "******* 0 41 [D loss: 1.029560, acc: 81.25%] [G loss: 8.886281]\n",
            "******* 0 42 [D loss: 0.050513, acc: 96.88%] [G loss: 15.281218]\n",
            "******* 0 43 [D loss: 0.000041, acc: 100.00%] [G loss: 24.829741]\n",
            "******* 0 44 [D loss: 10.371574, acc: 81.25%] [G loss: 9.937271]\n",
            "******* 0 45 [D loss: 1.526375, acc: 90.62%] [G loss: 8.992034]\n",
            "******* 0 46 [D loss: 0.448826, acc: 96.88%] [G loss: 9.586902]\n",
            "******* 0 47 [D loss: 0.781693, acc: 93.75%] [G loss: 9.401602]\n",
            "******* 0 48 [D loss: 4.603627, acc: 78.12%] [G loss: 7.510947]\n",
            "******* 0 49 [D loss: 15.425051, acc: 62.50%] [G loss: 3.526892]\n",
            "******* 0 50 [D loss: 17.031635, acc: 71.88%] [G loss: 2.689281]\n",
            "******* 0 51 [D loss: 8.659955, acc: 71.88%] [G loss: 8.587002]\n",
            "******* 0 52 [D loss: 0.118217, acc: 96.88%] [G loss: 27.524921]\n",
            "******* 0 53 [D loss: 0.000000, acc: 100.00%] [G loss: 41.860695]\n",
            "******* 0 54 [D loss: 0.000432, acc: 100.00%] [G loss: 57.615105]\n",
            "******* 0 55 [D loss: 0.594256, acc: 90.62%] [G loss: 57.366943]\n",
            "******* 0 56 [D loss: 0.296839, acc: 96.88%] [G loss: 50.121277]\n",
            "******* 0 57 [D loss: 0.000000, acc: 100.00%] [G loss: 62.778568]\n",
            "******* 0 58 [D loss: 0.000000, acc: 100.00%] [G loss: 53.744225]\n",
            "******* 0 59 [D loss: 0.036056, acc: 96.88%] [G loss: 37.731705]\n",
            "******* 0 60 [D loss: 0.000000, acc: 100.00%] [G loss: 30.997223]\n",
            "******* 0 61 [D loss: 0.000000, acc: 100.00%] [G loss: 27.502167]\n",
            "******* 0 62 [D loss: 0.000000, acc: 100.00%] [G loss: 20.059473]\n",
            "******* 0 63 [D loss: 0.006240, acc: 100.00%] [G loss: 14.111621]\n",
            "******* 0 64 [D loss: 0.005390, acc: 100.00%] [G loss: 6.840322]\n",
            "******* 0 65 [D loss: 0.271177, acc: 87.50%] [G loss: 2.689012]\n",
            "******* 0 66 [D loss: 0.334652, acc: 87.50%] [G loss: 4.255463]\n",
            "******* 0 67 [D loss: 0.059378, acc: 96.88%] [G loss: 7.903524]\n",
            "******* 0 68 [D loss: 0.017848, acc: 100.00%] [G loss: 11.726706]\n",
            "******* 0 69 [D loss: 0.000061, acc: 100.00%] [G loss: 15.643487]\n",
            "******* 0 70 [D loss: 0.000004, acc: 100.00%] [G loss: 18.649733]\n",
            "******* 0 71 [D loss: 0.000000, acc: 100.00%] [G loss: 20.989279]\n",
            "******* 0 72 [D loss: 0.000000, acc: 100.00%] [G loss: 25.050501]\n",
            "******* 0 73 [D loss: 0.000001, acc: 100.00%] [G loss: 24.961010]\n",
            "******* 0 74 [D loss: 0.000000, acc: 100.00%] [G loss: 27.668381]\n",
            "******* 0 75 [D loss: 0.000000, acc: 100.00%] [G loss: 27.789036]\n",
            "******* 0 76 [D loss: 0.000000, acc: 100.00%] [G loss: 27.138136]\n",
            "******* 0 77 [D loss: 0.000000, acc: 100.00%] [G loss: 25.250528]\n",
            "******* 0 78 [D loss: 0.000000, acc: 100.00%] [G loss: 20.346409]\n",
            "******* 0 79 [D loss: 0.000007, acc: 100.00%] [G loss: 15.403095]\n",
            "******* 0 80 [D loss: 0.153971, acc: 93.75%] [G loss: 23.076208]\n",
            "******* 0 81 [D loss: 0.005701, acc: 100.00%] [G loss: 32.496738]\n",
            "******* 0 82 [D loss: 0.001127, acc: 100.00%] [G loss: 39.810379]\n",
            "******* 0 83 [D loss: 0.000455, acc: 100.00%] [G loss: 45.462708]\n",
            "******* 0 84 [D loss: 0.000063, acc: 100.00%] [G loss: 50.606667]\n",
            "******* 0 85 [D loss: 0.000001, acc: 100.00%] [G loss: 54.354958]\n",
            "******* 0 86 [D loss: 0.000170, acc: 100.00%] [G loss: 57.585236]\n",
            "******* 0 87 [D loss: 0.040968, acc: 96.88%] [G loss: 58.803944]\n",
            "******* 0 88 [D loss: 0.013601, acc: 100.00%] [G loss: 61.873745]\n",
            "******* 0 89 [D loss: 0.087902, acc: 96.88%] [G loss: 58.939888]\n",
            "******* 0 90 [D loss: 0.094565, acc: 96.88%] [G loss: 50.315113]\n",
            "******* 0 91 [D loss: 0.215279, acc: 96.88%] [G loss: 34.861370]\n",
            "******* 0 92 [D loss: 0.000000, acc: 100.00%] [G loss: 22.360472]\n",
            "******* 0 93 [D loss: 0.000001, acc: 100.00%] [G loss: 10.001591]\n",
            "******* 0 94 [D loss: 0.006041, acc: 100.00%] [G loss: 4.811676]\n",
            "******* 0 95 [D loss: 0.122258, acc: 100.00%] [G loss: 21.317856]\n",
            "******* 0 96 [D loss: 0.000000, acc: 100.00%] [G loss: 54.499916]\n",
            "******* 0 97 [D loss: 0.000000, acc: 100.00%] [G loss: 77.685097]\n",
            "******* 0 98 [D loss: 0.000000, acc: 100.00%] [G loss: 96.473213]\n",
            "******* 0 99 [D loss: 1.039776, acc: 93.75%] [G loss: 103.525291]\n",
            "******* 0 100 [D loss: 0.000000, acc: 100.00%] [G loss: 99.604317]\n",
            "******* 0 101 [D loss: 0.000000, acc: 100.00%] [G loss: 102.264481]\n",
            "******* 0 102 [D loss: 0.000000, acc: 100.00%] [G loss: 95.777718]\n",
            "******* 0 103 [D loss: 0.000000, acc: 100.00%] [G loss: 93.608124]\n",
            "******* 0 104 [D loss: 0.000001, acc: 100.00%] [G loss: 83.928055]\n",
            "******* 0 105 [D loss: 0.000000, acc: 100.00%] [G loss: 84.141632]\n",
            "******* 0 106 [D loss: 0.000000, acc: 100.00%] [G loss: 78.621605]\n",
            "******* 0 107 [D loss: 0.000000, acc: 100.00%] [G loss: 70.632858]\n",
            "******* 0 108 [D loss: 0.070353, acc: 96.88%] [G loss: 52.369873]\n",
            "******* 0 109 [D loss: 0.000000, acc: 100.00%] [G loss: 33.062321]\n",
            "******* 0 110 [D loss: 0.001434, acc: 100.00%] [G loss: 18.263401]\n",
            "******* 0 111 [D loss: 3.719481, acc: 59.38%] [G loss: 33.985100]\n",
            "******* 0 112 [D loss: 0.000000, acc: 100.00%] [G loss: 89.191696]\n",
            "******* 0 113 [D loss: 0.000000, acc: 100.00%] [G loss: 126.700630]\n",
            "******* 0 114 [D loss: 0.000000, acc: 100.00%] [G loss: 152.893829]\n",
            "******* 0 115 [D loss: 4.003388, acc: 90.62%] [G loss: 98.675964]\n",
            "******* 0 116 [D loss: 0.188225, acc: 96.88%] [G loss: 42.845997]\n",
            "******* 0 117 [D loss: 6.975257, acc: 90.62%] [G loss: 9.491937]\n",
            "******* 0 118 [D loss: 2.666622, acc: 65.62%] [G loss: 7.937519]\n",
            "******* 0 119 [D loss: 0.000066, acc: 100.00%] [G loss: 18.543209]\n",
            "******* 0 120 [D loss: 1.267552, acc: 93.75%] [G loss: 23.200386]\n",
            "******* 0 121 [D loss: 2.997057, acc: 93.75%] [G loss: 28.149643]\n",
            "******* 0 122 [D loss: 6.450777, acc: 87.50%] [G loss: 30.304752]\n",
            "******* 0 123 [D loss: 2.239326, acc: 93.75%] [G loss: 29.640406]\n",
            "******* 0 124 [D loss: 0.000001, acc: 100.00%] [G loss: 29.218925]\n",
            "******* 0 125 [D loss: 0.000001, acc: 100.00%] [G loss: 27.407097]\n",
            "******* 0 126 [D loss: 0.427700, acc: 96.88%] [G loss: 22.004715]\n",
            "******* 0 127 [D loss: 0.295120, acc: 93.75%] [G loss: 14.272337]\n",
            "******* 0 128 [D loss: 0.690768, acc: 75.00%] [G loss: 8.295043]\n",
            "******* 0 129 [D loss: 1.034598, acc: 87.50%] [G loss: 13.381735]\n",
            "******* 0 130 [D loss: 0.029495, acc: 96.88%] [G loss: 20.252094]\n",
            "******* 0 131 [D loss: 1.093761, acc: 90.62%] [G loss: 22.198271]\n",
            "******* 0 132 [D loss: 0.199763, acc: 93.75%] [G loss: 26.053690]\n",
            "******* 0 133 [D loss: 0.060168, acc: 96.88%] [G loss: 29.593418]\n",
            "******* 0 134 [D loss: 1.079603, acc: 87.50%] [G loss: 24.921356]\n",
            "******* 0 135 [D loss: 0.030412, acc: 96.88%] [G loss: 21.986584]\n",
            "******* 0 136 [D loss: 0.529447, acc: 93.75%] [G loss: 19.440731]\n",
            "******* 0 137 [D loss: 0.968710, acc: 90.62%] [G loss: 14.339272]\n",
            "******* 0 138 [D loss: 0.805127, acc: 93.75%] [G loss: 7.002633]\n",
            "******* 0 139 [D loss: 0.060561, acc: 96.88%] [G loss: 8.085150]\n",
            "******* 0 140 [D loss: 0.003475, acc: 100.00%] [G loss: 8.739354]\n",
            "******* 0 141 [D loss: 0.034928, acc: 100.00%] [G loss: 11.219360]\n",
            "******* 0 142 [D loss: 0.158295, acc: 96.88%] [G loss: 10.761486]\n",
            "******* 0 143 [D loss: 0.040469, acc: 96.88%] [G loss: 14.081352]\n",
            "******* 0 144 [D loss: 0.303076, acc: 96.88%] [G loss: 20.547012]\n",
            "******* 0 145 [D loss: 0.635074, acc: 93.75%] [G loss: 22.200373]\n",
            "******* 0 146 [D loss: 0.000000, acc: 100.00%] [G loss: 23.594570]\n",
            "******* 0 147 [D loss: 0.290629, acc: 96.88%] [G loss: 24.250793]\n",
            "******* 0 148 [D loss: 0.000000, acc: 100.00%] [G loss: 23.576317]\n",
            "******* 0 149 [D loss: 0.000004, acc: 100.00%] [G loss: 25.159876]\n",
            "******* 0 150 [D loss: 0.000104, acc: 100.00%] [G loss: 23.279903]\n",
            "******* 0 151 [D loss: 0.428611, acc: 96.88%] [G loss: 23.759354]\n",
            "******* 0 152 [D loss: 0.044419, acc: 96.88%] [G loss: 11.715788]\n",
            "******* 0 153 [D loss: 0.537553, acc: 96.88%] [G loss: 12.723014]\n",
            "******* 0 154 [D loss: 0.003876, acc: 100.00%] [G loss: 8.600841]\n",
            "******* 0 155 [D loss: 0.138691, acc: 96.88%] [G loss: 9.811625]\n",
            "******* 0 156 [D loss: 0.000614, acc: 100.00%] [G loss: 14.098952]\n",
            "******* 0 157 [D loss: 0.436980, acc: 96.88%] [G loss: 16.446939]\n",
            "******* 0 158 [D loss: 0.636373, acc: 96.88%] [G loss: 15.166254]\n",
            "******* 0 159 [D loss: 0.191771, acc: 93.75%] [G loss: 10.075916]\n",
            "******* 0 160 [D loss: 0.676668, acc: 93.75%] [G loss: 3.449280]\n",
            "******* 0 161 [D loss: 0.900609, acc: 75.00%] [G loss: 10.094862]\n",
            "******* 0 162 [D loss: 0.003803, acc: 100.00%] [G loss: 25.003147]\n",
            "******* 1 0 [D loss: 0.017240, acc: 100.00%] [G loss: 30.025211]\n",
            "******* 1 1 [D loss: 1.506501, acc: 93.75%] [G loss: 30.101416]\n",
            "******* 1 2 [D loss: 1.786728, acc: 93.75%] [G loss: 29.165398]\n",
            "******* 1 3 [D loss: 1.076478, acc: 87.50%] [G loss: 22.946428]\n",
            "******* 1 4 [D loss: 0.424957, acc: 96.88%] [G loss: 20.647964]\n",
            "******* 1 5 [D loss: 0.405812, acc: 93.75%] [G loss: 14.806484]\n",
            "******* 1 6 [D loss: 0.174375, acc: 96.88%] [G loss: 6.701472]\n",
            "******* 1 7 [D loss: 1.727944, acc: 68.75%] [G loss: 9.754166]\n",
            "******* 1 8 [D loss: 1.201077, acc: 81.25%] [G loss: 13.156104]\n",
            "******* 1 9 [D loss: 0.446334, acc: 90.62%] [G loss: 16.119780]\n",
            "******* 1 10 [D loss: 1.022338, acc: 87.50%] [G loss: 12.394051]\n",
            "******* 1 11 [D loss: 0.026695, acc: 96.88%] [G loss: 13.445252]\n",
            "******* 1 12 [D loss: 0.563386, acc: 90.62%] [G loss: 8.430586]\n",
            "******* 1 13 [D loss: 0.018848, acc: 100.00%] [G loss: 7.721740]\n",
            "******* 1 14 [D loss: 0.885478, acc: 96.88%] [G loss: 5.116803]\n",
            "******* 1 15 [D loss: 0.184486, acc: 90.62%] [G loss: 3.141426]\n",
            "******* 1 16 [D loss: 0.102366, acc: 100.00%] [G loss: 2.687878]\n",
            "******* 1 17 [D loss: 0.121488, acc: 90.62%] [G loss: 3.035167]\n",
            "******* 1 18 [D loss: 0.042377, acc: 100.00%] [G loss: 4.967758]\n",
            "******* 1 19 [D loss: 0.029531, acc: 100.00%] [G loss: 4.757053]\n",
            "******* 1 20 [D loss: 0.074204, acc: 96.88%] [G loss: 5.410277]\n",
            "******* 1 21 [D loss: 0.046657, acc: 100.00%] [G loss: 5.645602]\n",
            "******* 1 22 [D loss: 0.009513, acc: 100.00%] [G loss: 4.574259]\n",
            "******* 1 23 [D loss: 0.120604, acc: 96.88%] [G loss: 4.514345]\n",
            "******* 1 24 [D loss: 0.088257, acc: 96.88%] [G loss: 4.972366]\n",
            "******* 1 25 [D loss: 0.053856, acc: 93.75%] [G loss: 5.754866]\n",
            "******* 1 26 [D loss: 0.046297, acc: 100.00%] [G loss: 5.721264]\n",
            "******* 1 27 [D loss: 0.008060, acc: 100.00%] [G loss: 5.331474]\n",
            "******* 1 28 [D loss: 0.117583, acc: 93.75%] [G loss: 5.512039]\n",
            "******* 1 29 [D loss: 0.233785, acc: 96.88%] [G loss: 4.096783]\n",
            "******* 1 30 [D loss: 0.154787, acc: 93.75%] [G loss: 4.005979]\n",
            "******* 1 31 [D loss: 0.183258, acc: 90.62%] [G loss: 4.311218]\n",
            "******* 1 32 [D loss: 0.024366, acc: 100.00%] [G loss: 4.853320]\n",
            "******* 1 33 [D loss: 0.024075, acc: 100.00%] [G loss: 4.998848]\n",
            "******* 1 34 [D loss: 0.083152, acc: 96.88%] [G loss: 5.230429]\n",
            "******* 1 35 [D loss: 0.040914, acc: 96.88%] [G loss: 4.812938]\n",
            "******* 1 36 [D loss: 0.188680, acc: 96.88%] [G loss: 3.874732]\n",
            "******* 1 37 [D loss: 0.017292, acc: 100.00%] [G loss: 3.813340]\n",
            "******* 1 38 [D loss: 0.025612, acc: 100.00%] [G loss: 4.829911]\n",
            "******* 1 39 [D loss: 0.168478, acc: 93.75%] [G loss: 4.393228]\n",
            "******* 1 40 [D loss: 0.015732, acc: 100.00%] [G loss: 4.429276]\n",
            "******* 1 41 [D loss: 0.020987, acc: 100.00%] [G loss: 4.634242]\n",
            "******* 1 42 [D loss: 0.032594, acc: 100.00%] [G loss: 5.544674]\n",
            "******* 1 43 [D loss: 0.057936, acc: 96.88%] [G loss: 4.998322]\n",
            "******* 1 44 [D loss: 0.100974, acc: 96.88%] [G loss: 4.737775]\n",
            "******* 1 45 [D loss: 0.028563, acc: 100.00%] [G loss: 4.646463]\n",
            "******* 1 46 [D loss: 0.030112, acc: 100.00%] [G loss: 4.330013]\n",
            "******* 1 47 [D loss: 0.200933, acc: 96.88%] [G loss: 4.494713]\n",
            "******* 1 48 [D loss: 0.028475, acc: 100.00%] [G loss: 4.116125]\n",
            "******* 1 49 [D loss: 0.109417, acc: 96.88%] [G loss: 3.759261]\n",
            "******* 1 50 [D loss: 0.044344, acc: 96.88%] [G loss: 3.375427]\n",
            "******* 1 51 [D loss: 0.078065, acc: 96.88%] [G loss: 4.170042]\n",
            "******* 1 52 [D loss: 0.018361, acc: 100.00%] [G loss: 4.289442]\n",
            "******* 1 53 [D loss: 0.112820, acc: 96.88%] [G loss: 4.143051]\n",
            "******* 1 54 [D loss: 0.033191, acc: 100.00%] [G loss: 5.756428]\n",
            "******* 1 55 [D loss: 0.029098, acc: 100.00%] [G loss: 6.489637]\n",
            "******* 1 56 [D loss: 0.001432, acc: 100.00%] [G loss: 6.760395]\n",
            "******* 1 57 [D loss: 0.119567, acc: 96.88%] [G loss: 6.386127]\n",
            "******* 1 58 [D loss: 0.107101, acc: 93.75%] [G loss: 6.130781]\n",
            "******* 1 59 [D loss: 0.021906, acc: 100.00%] [G loss: 6.687843]\n",
            "******* 1 60 [D loss: 0.007075, acc: 100.00%] [G loss: 7.082257]\n",
            "******* 1 61 [D loss: 0.001786, acc: 100.00%] [G loss: 7.880672]\n",
            "******* 1 62 [D loss: 0.279903, acc: 87.50%] [G loss: 8.981037]\n",
            "******* 1 63 [D loss: 0.004148, acc: 100.00%] [G loss: 8.361309]\n",
            "******* 1 64 [D loss: 0.041777, acc: 96.88%] [G loss: 8.222913]\n",
            "******* 1 65 [D loss: 0.585767, acc: 87.50%] [G loss: 19.151400]\n",
            "******* 1 66 [D loss: 0.038352, acc: 96.88%] [G loss: 34.243111]\n",
            "******* 1 67 [D loss: 0.022904, acc: 100.00%] [G loss: 43.425694]\n",
            "******* 1 68 [D loss: 0.868518, acc: 90.62%] [G loss: 43.578857]\n",
            "******* 1 69 [D loss: 0.071159, acc: 96.88%] [G loss: 42.960579]\n",
            "******* 1 70 [D loss: 0.277964, acc: 96.88%] [G loss: 34.246891]\n",
            "******* 1 71 [D loss: 0.324585, acc: 96.88%] [G loss: 17.863237]\n",
            "******* 1 72 [D loss: 0.001254, acc: 100.00%] [G loss: 8.752714]\n",
            "******* 1 73 [D loss: 0.033710, acc: 100.00%] [G loss: 3.036026]\n",
            "******* 1 74 [D loss: 0.852940, acc: 71.88%] [G loss: 2.263623]\n",
            "******* 1 75 [D loss: 0.837651, acc: 71.88%] [G loss: 3.054302]\n",
            "******* 1 76 [D loss: 0.180120, acc: 93.75%] [G loss: 6.746359]\n",
            "******* 1 77 [D loss: 0.008926, acc: 100.00%] [G loss: 8.906870]\n",
            "******* 1 78 [D loss: 0.889329, acc: 84.38%] [G loss: 8.487920]\n",
            "******* 1 79 [D loss: 0.007696, acc: 100.00%] [G loss: 7.985632]\n",
            "******* 1 80 [D loss: 0.010734, acc: 100.00%] [G loss: 7.085918]\n",
            "******* 1 81 [D loss: 0.167038, acc: 93.75%] [G loss: 6.412244]\n",
            "******* 1 82 [D loss: 0.063661, acc: 93.75%] [G loss: 7.232968]\n",
            "******* 1 83 [D loss: 0.100740, acc: 96.88%] [G loss: 7.703154]\n",
            "******* 1 84 [D loss: 0.010368, acc: 100.00%] [G loss: 8.170408]\n",
            "******* 1 85 [D loss: 0.015801, acc: 100.00%] [G loss: 8.223516]\n",
            "******* 1 86 [D loss: 0.101508, acc: 96.88%] [G loss: 8.364278]\n",
            "******* 1 87 [D loss: 0.344184, acc: 93.75%] [G loss: 6.595785]\n",
            "******* 1 88 [D loss: 0.073592, acc: 93.75%] [G loss: 5.384550]\n",
            "******* 1 89 [D loss: 0.127705, acc: 96.88%] [G loss: 3.305614]\n",
            "******* 1 90 [D loss: 0.045884, acc: 100.00%] [G loss: 4.031596]\n",
            "******* 1 91 [D loss: 0.061263, acc: 100.00%] [G loss: 5.662938]\n",
            "******* 1 92 [D loss: 0.047046, acc: 96.88%] [G loss: 5.558410]\n",
            "******* 1 93 [D loss: 0.014417, acc: 100.00%] [G loss: 6.790858]\n",
            "******* 1 94 [D loss: 0.024653, acc: 100.00%] [G loss: 7.046474]\n",
            "******* 1 95 [D loss: 0.004331, acc: 100.00%] [G loss: 6.600973]\n",
            "******* 1 96 [D loss: 0.812303, acc: 78.12%] [G loss: 10.290718]\n",
            "******* 1 97 [D loss: 0.208587, acc: 96.88%] [G loss: 14.361126]\n",
            "******* 1 98 [D loss: 0.104322, acc: 96.88%] [G loss: 15.586835]\n",
            "******* 1 99 [D loss: 0.097729, acc: 96.88%] [G loss: 16.455135]\n",
            "******* 1 100 [D loss: 1.110576, acc: 87.50%] [G loss: 16.052158]\n",
            "******* 1 101 [D loss: 1.197889, acc: 90.62%] [G loss: 19.247658]\n",
            "******* 1 102 [D loss: 1.166449, acc: 81.25%] [G loss: 16.769163]\n",
            "******* 1 103 [D loss: 1.781811, acc: 75.00%] [G loss: 12.601055]\n",
            "******* 1 104 [D loss: 1.059743, acc: 81.25%] [G loss: 8.742624]\n",
            "******* 1 105 [D loss: 0.796354, acc: 84.38%] [G loss: 8.949146]\n",
            "******* 1 106 [D loss: 0.571917, acc: 87.50%] [G loss: 11.676022]\n",
            "******* 1 107 [D loss: 0.121362, acc: 93.75%] [G loss: 9.315842]\n",
            "******* 1 108 [D loss: 1.130382, acc: 81.25%] [G loss: 3.402580]\n",
            "******* 1 109 [D loss: 0.531225, acc: 75.00%] [G loss: 2.502140]\n",
            "******* 1 110 [D loss: 0.575421, acc: 68.75%] [G loss: 3.019439]\n",
            "******* 1 111 [D loss: 0.193227, acc: 87.50%] [G loss: 5.514826]\n",
            "******* 1 112 [D loss: 0.474755, acc: 84.38%] [G loss: 6.352442]\n",
            "******* 1 113 [D loss: 0.402701, acc: 87.50%] [G loss: 6.269525]\n",
            "******* 1 114 [D loss: 0.370692, acc: 87.50%] [G loss: 5.354764]\n",
            "******* 1 115 [D loss: 0.123482, acc: 96.88%] [G loss: 4.147378]\n",
            "******* 1 116 [D loss: 0.070438, acc: 100.00%] [G loss: 4.477399]\n",
            "******* 1 117 [D loss: 0.159740, acc: 96.88%] [G loss: 6.065870]\n",
            "******* 1 118 [D loss: 0.033358, acc: 100.00%] [G loss: 5.179912]\n",
            "******* 1 119 [D loss: 0.146486, acc: 96.88%] [G loss: 7.311908]\n",
            "******* 1 120 [D loss: 0.429628, acc: 90.62%] [G loss: 5.890070]\n",
            "******* 1 121 [D loss: 0.108448, acc: 96.88%] [G loss: 5.858721]\n",
            "******* 1 122 [D loss: 0.846704, acc: 87.50%] [G loss: 5.141375]\n",
            "******* 1 123 [D loss: 0.282697, acc: 84.38%] [G loss: 2.352467]\n",
            "******* 1 124 [D loss: 0.184898, acc: 90.62%] [G loss: 2.788013]\n",
            "******* 1 125 [D loss: 0.101449, acc: 96.88%] [G loss: 3.232920]\n",
            "******* 1 126 [D loss: 0.150173, acc: 93.75%] [G loss: 5.339112]\n",
            "******* 1 127 [D loss: 0.049596, acc: 100.00%] [G loss: 6.635763]\n",
            "******* 1 128 [D loss: 0.221277, acc: 90.62%] [G loss: 5.067727]\n",
            "******* 1 129 [D loss: 0.282036, acc: 90.62%] [G loss: 4.777437]\n",
            "******* 1 130 [D loss: 0.239700, acc: 84.38%] [G loss: 2.971520]\n",
            "******* 1 131 [D loss: 0.334379, acc: 90.62%] [G loss: 2.064492]\n",
            "******* 1 132 [D loss: 0.212684, acc: 93.75%] [G loss: 2.911249]\n",
            "******* 1 133 [D loss: 0.052261, acc: 100.00%] [G loss: 3.546728]\n",
            "******* 1 134 [D loss: 0.072487, acc: 96.88%] [G loss: 5.260352]\n",
            "******* 1 135 [D loss: 0.065832, acc: 96.88%] [G loss: 4.971553]\n",
            "******* 1 136 [D loss: 0.069454, acc: 96.88%] [G loss: 5.575839]\n",
            "******* 1 137 [D loss: 0.085142, acc: 96.88%] [G loss: 5.844866]\n",
            "******* 1 138 [D loss: 0.107739, acc: 93.75%] [G loss: 4.821838]\n",
            "******* 1 139 [D loss: 0.110818, acc: 96.88%] [G loss: 3.672664]\n",
            "******* 1 140 [D loss: 0.056830, acc: 96.88%] [G loss: 3.523492]\n",
            "******* 1 141 [D loss: 0.024126, acc: 100.00%] [G loss: 3.475221]\n",
            "******* 1 142 [D loss: 0.061437, acc: 100.00%] [G loss: 4.062233]\n",
            "******* 1 143 [D loss: 0.056907, acc: 100.00%] [G loss: 3.693202]\n",
            "******* 1 144 [D loss: 0.075560, acc: 96.88%] [G loss: 4.314834]\n",
            "******* 1 145 [D loss: 0.018270, acc: 100.00%] [G loss: 4.873807]\n",
            "******* 1 146 [D loss: 0.055306, acc: 100.00%] [G loss: 4.735832]\n",
            "******* 1 147 [D loss: 0.023204, acc: 100.00%] [G loss: 4.438295]\n",
            "******* 1 148 [D loss: 0.026538, acc: 100.00%] [G loss: 4.749023]\n",
            "******* 1 149 [D loss: 0.014753, acc: 100.00%] [G loss: 4.912304]\n",
            "******* 1 150 [D loss: 0.018350, acc: 100.00%] [G loss: 5.118966]\n",
            "******* 1 151 [D loss: 0.033088, acc: 100.00%] [G loss: 5.348790]\n",
            "******* 1 152 [D loss: 0.009495, acc: 100.00%] [G loss: 5.315704]\n",
            "******* 1 153 [D loss: 0.018595, acc: 100.00%] [G loss: 5.132462]\n",
            "******* 1 154 [D loss: 0.013871, acc: 100.00%] [G loss: 6.045282]\n",
            "******* 1 155 [D loss: 0.013442, acc: 100.00%] [G loss: 5.368343]\n",
            "******* 1 156 [D loss: 0.007952, acc: 100.00%] [G loss: 5.769193]\n",
            "******* 1 157 [D loss: 0.006930, acc: 100.00%] [G loss: 5.834104]\n",
            "******* 1 158 [D loss: 0.003444, acc: 100.00%] [G loss: 5.555300]\n",
            "******* 1 159 [D loss: 0.008486, acc: 100.00%] [G loss: 6.877418]\n",
            "******* 1 160 [D loss: 0.035763, acc: 100.00%] [G loss: 5.865326]\n",
            "******* 1 161 [D loss: 0.025661, acc: 100.00%] [G loss: 5.112965]\n",
            "******* 1 162 [D loss: 0.049503, acc: 96.88%] [G loss: 5.741067]\n",
            "******* 2 0 [D loss: 0.061504, acc: 96.88%] [G loss: 4.939192]\n",
            "******* 2 1 [D loss: 0.043764, acc: 100.00%] [G loss: 4.780849]\n",
            "******* 2 2 [D loss: 0.035478, acc: 100.00%] [G loss: 4.767616]\n",
            "******* 2 3 [D loss: 0.013024, acc: 100.00%] [G loss: 6.432682]\n",
            "******* 2 4 [D loss: 0.013916, acc: 100.00%] [G loss: 7.101440]\n",
            "******* 2 5 [D loss: 0.012959, acc: 100.00%] [G loss: 7.965865]\n",
            "******* 2 6 [D loss: 0.012637, acc: 100.00%] [G loss: 7.685736]\n",
            "******* 2 7 [D loss: 0.009619, acc: 100.00%] [G loss: 8.096786]\n",
            "******* 2 8 [D loss: 0.008476, acc: 100.00%] [G loss: 8.307478]\n",
            "******* 2 9 [D loss: 0.030637, acc: 96.88%] [G loss: 7.272678]\n",
            "******* 2 10 [D loss: 0.013316, acc: 100.00%] [G loss: 6.743229]\n",
            "******* 2 11 [D loss: 0.026683, acc: 100.00%] [G loss: 6.542969]\n",
            "******* 2 12 [D loss: 0.025498, acc: 100.00%] [G loss: 6.236487]\n",
            "******* 2 13 [D loss: 0.039766, acc: 96.88%] [G loss: 6.299524]\n",
            "******* 2 14 [D loss: 0.006067, acc: 100.00%] [G loss: 4.870096]\n",
            "******* 2 15 [D loss: 0.021613, acc: 100.00%] [G loss: 5.474066]\n",
            "******* 2 16 [D loss: 0.004706, acc: 100.00%] [G loss: 5.610788]\n",
            "******* 2 17 [D loss: 0.012524, acc: 100.00%] [G loss: 4.870995]\n",
            "******* 2 18 [D loss: 0.043272, acc: 96.88%] [G loss: 4.495978]\n",
            "******* 2 19 [D loss: 0.011644, acc: 100.00%] [G loss: 4.731571]\n",
            "******* 2 20 [D loss: 0.043126, acc: 96.88%] [G loss: 5.358293]\n",
            "******* 2 21 [D loss: 0.009488, acc: 100.00%] [G loss: 4.804533]\n",
            "******* 2 22 [D loss: 0.009243, acc: 100.00%] [G loss: 5.238050]\n",
            "******* 2 23 [D loss: 0.012665, acc: 100.00%] [G loss: 5.937301]\n",
            "******* 2 24 [D loss: 0.010262, acc: 100.00%] [G loss: 5.918427]\n",
            "******* 2 25 [D loss: 0.004431, acc: 100.00%] [G loss: 6.322966]\n",
            "******* 2 26 [D loss: 0.004045, acc: 100.00%] [G loss: 6.001611]\n",
            "******* 2 27 [D loss: 0.008089, acc: 100.00%] [G loss: 6.513769]\n",
            "******* 2 28 [D loss: 0.005571, acc: 100.00%] [G loss: 5.769166]\n",
            "******* 2 29 [D loss: 0.013684, acc: 100.00%] [G loss: 5.788687]\n",
            "******* 2 30 [D loss: 0.082970, acc: 96.88%] [G loss: 6.178454]\n",
            "******* 2 31 [D loss: 0.033097, acc: 96.88%] [G loss: 6.657522]\n",
            "******* 2 32 [D loss: 0.075826, acc: 96.88%] [G loss: 7.918056]\n",
            "******* 2 33 [D loss: 0.017663, acc: 100.00%] [G loss: 9.300531]\n",
            "******* 2 34 [D loss: 0.053200, acc: 96.88%] [G loss: 9.812418]\n",
            "******* 2 35 [D loss: 0.783229, acc: 84.38%] [G loss: 6.432385]\n",
            "******* 2 36 [D loss: 0.339871, acc: 87.50%] [G loss: 4.640997]\n",
            "******* 2 37 [D loss: 0.248815, acc: 93.75%] [G loss: 3.978280]\n",
            "******* 2 38 [D loss: 0.028647, acc: 100.00%] [G loss: 4.791020]\n",
            "******* 2 39 [D loss: 0.033799, acc: 100.00%] [G loss: 4.447049]\n",
            "******* 2 40 [D loss: 0.018587, acc: 100.00%] [G loss: 5.106287]\n",
            "******* 2 41 [D loss: 0.021361, acc: 100.00%] [G loss: 5.055847]\n",
            "******* 2 42 [D loss: 0.033702, acc: 100.00%] [G loss: 4.378726]\n",
            "******* 2 43 [D loss: 0.021297, acc: 100.00%] [G loss: 5.519441]\n",
            "******* 2 44 [D loss: 0.026741, acc: 100.00%] [G loss: 5.411355]\n",
            "******* 2 45 [D loss: 0.153176, acc: 93.75%] [G loss: 4.773861]\n",
            "******* 2 46 [D loss: 0.138653, acc: 90.62%] [G loss: 3.709383]\n",
            "******* 2 47 [D loss: 0.058628, acc: 100.00%] [G loss: 4.715141]\n",
            "******* 2 48 [D loss: 0.188339, acc: 93.75%] [G loss: 4.218513]\n",
            "******* 2 49 [D loss: 0.119018, acc: 96.88%] [G loss: 3.393258]\n",
            "******* 2 50 [D loss: 0.080766, acc: 96.88%] [G loss: 4.620893]\n",
            "******* 2 51 [D loss: 0.010940, acc: 100.00%] [G loss: 5.038992]\n",
            "******* 2 52 [D loss: 0.098551, acc: 96.88%] [G loss: 5.241024]\n",
            "******* 2 53 [D loss: 0.008749, acc: 100.00%] [G loss: 4.993156]\n",
            "******* 2 54 [D loss: 0.017273, acc: 100.00%] [G loss: 5.272066]\n",
            "******* 2 55 [D loss: 0.086771, acc: 96.88%] [G loss: 4.067601]\n",
            "******* 2 56 [D loss: 0.067310, acc: 96.88%] [G loss: 2.920627]\n",
            "******* 2 57 [D loss: 0.180781, acc: 96.88%] [G loss: 3.906480]\n",
            "******* 2 58 [D loss: 0.099225, acc: 93.75%] [G loss: 5.753633]\n",
            "******* 2 59 [D loss: 0.034453, acc: 100.00%] [G loss: 7.603314]\n",
            "******* 2 60 [D loss: 0.091973, acc: 96.88%] [G loss: 6.864648]\n",
            "******* 2 61 [D loss: 0.009727, acc: 100.00%] [G loss: 6.211932]\n",
            "******* 2 62 [D loss: 0.186463, acc: 96.88%] [G loss: 5.026278]\n",
            "******* 2 63 [D loss: 0.018895, acc: 100.00%] [G loss: 4.708744]\n",
            "******* 2 64 [D loss: 0.179907, acc: 93.75%] [G loss: 2.891060]\n",
            "******* 2 65 [D loss: 0.146854, acc: 96.88%] [G loss: 2.278182]\n",
            "******* 2 66 [D loss: 0.108957, acc: 100.00%] [G loss: 3.261705]\n",
            "******* 2 67 [D loss: 0.051093, acc: 100.00%] [G loss: 4.091938]\n",
            "******* 2 68 [D loss: 0.025433, acc: 100.00%] [G loss: 5.148843]\n",
            "******* 2 69 [D loss: 0.007395, acc: 100.00%] [G loss: 5.966209]\n",
            "******* 2 70 [D loss: 0.178701, acc: 93.75%] [G loss: 5.997941]\n",
            "******* 2 71 [D loss: 0.025388, acc: 100.00%] [G loss: 5.389421]\n",
            "******* 2 72 [D loss: 0.009171, acc: 100.00%] [G loss: 5.311330]\n",
            "******* 2 73 [D loss: 0.007559, acc: 100.00%] [G loss: 5.190189]\n",
            "******* 2 74 [D loss: 0.021932, acc: 100.00%] [G loss: 4.567932]\n",
            "******* 2 75 [D loss: 0.089732, acc: 93.75%] [G loss: 3.617273]\n",
            "******* 2 76 [D loss: 0.038108, acc: 100.00%] [G loss: 3.297234]\n",
            "******* 2 77 [D loss: 0.033490, acc: 100.00%] [G loss: 3.520968]\n",
            "******* 2 78 [D loss: 0.072040, acc: 100.00%] [G loss: 4.038068]\n",
            "******* 2 79 [D loss: 0.025543, acc: 100.00%] [G loss: 4.177187]\n",
            "******* 2 80 [D loss: 0.018905, acc: 100.00%] [G loss: 6.058560]\n",
            "******* 2 81 [D loss: 0.005707, acc: 100.00%] [G loss: 5.997257]\n",
            "******* 2 82 [D loss: 0.002996, acc: 100.00%] [G loss: 7.734509]\n",
            "******* 2 83 [D loss: 0.314386, acc: 90.62%] [G loss: 5.613401]\n",
            "******* 2 84 [D loss: 0.044886, acc: 96.88%] [G loss: 4.290449]\n",
            "******* 2 85 [D loss: 0.150264, acc: 96.88%] [G loss: 4.312980]\n",
            "******* 2 86 [D loss: 0.022292, acc: 100.00%] [G loss: 4.631838]\n",
            "******* 2 87 [D loss: 0.004029, acc: 100.00%] [G loss: 5.557994]\n",
            "******* 2 88 [D loss: 0.079491, acc: 96.88%] [G loss: 6.352482]\n",
            "******* 2 89 [D loss: 0.045586, acc: 96.88%] [G loss: 6.514235]\n",
            "******* 2 90 [D loss: 0.055804, acc: 96.88%] [G loss: 5.290521]\n",
            "******* 2 91 [D loss: 0.154459, acc: 96.88%] [G loss: 4.075101]\n",
            "******* 2 92 [D loss: 0.057332, acc: 96.88%] [G loss: 3.987571]\n",
            "******* 2 93 [D loss: 0.020643, acc: 100.00%] [G loss: 2.606489]\n",
            "******* 2 94 [D loss: 0.056217, acc: 100.00%] [G loss: 4.863190]\n",
            "******* 2 95 [D loss: 0.070270, acc: 96.88%] [G loss: 3.453985]\n",
            "******* 2 96 [D loss: 0.078352, acc: 96.88%] [G loss: 4.132067]\n",
            "******* 2 97 [D loss: 0.031917, acc: 100.00%] [G loss: 4.274413]\n",
            "******* 2 98 [D loss: 0.028089, acc: 100.00%] [G loss: 4.587173]\n",
            "******* 2 99 [D loss: 0.010119, acc: 100.00%] [G loss: 4.895003]\n",
            "******* 2 100 [D loss: 0.075407, acc: 96.88%] [G loss: 4.884895]\n",
            "******* 2 101 [D loss: 0.035511, acc: 96.88%] [G loss: 4.468041]\n",
            "******* 2 102 [D loss: 0.025539, acc: 100.00%] [G loss: 4.568339]\n",
            "******* 2 103 [D loss: 0.051117, acc: 96.88%] [G loss: 4.000096]\n",
            "******* 2 104 [D loss: 0.084174, acc: 96.88%] [G loss: 3.892467]\n",
            "******* 2 105 [D loss: 0.038568, acc: 100.00%] [G loss: 3.241242]\n",
            "******* 2 106 [D loss: 0.035828, acc: 100.00%] [G loss: 4.558525]\n",
            "******* 2 107 [D loss: 0.024221, acc: 100.00%] [G loss: 4.546708]\n",
            "******* 2 108 [D loss: 0.230431, acc: 90.62%] [G loss: 4.905457]\n",
            "******* 2 109 [D loss: 0.048284, acc: 100.00%] [G loss: 3.759181]\n",
            "******* 2 110 [D loss: 0.139177, acc: 93.75%] [G loss: 4.460367]\n",
            "******* 2 111 [D loss: 0.126038, acc: 93.75%] [G loss: 3.272280]\n",
            "******* 2 112 [D loss: 0.079904, acc: 96.88%] [G loss: 3.778595]\n",
            "******* 2 113 [D loss: 0.015311, acc: 100.00%] [G loss: 5.577147]\n",
            "******* 2 114 [D loss: 0.096802, acc: 93.75%] [G loss: 5.148839]\n",
            "******* 2 115 [D loss: 0.312093, acc: 87.50%] [G loss: 3.164962]\n",
            "******* 2 116 [D loss: 0.069281, acc: 96.88%] [G loss: 3.594000]\n",
            "******* 2 117 [D loss: 0.075311, acc: 96.88%] [G loss: 4.583250]\n",
            "******* 2 118 [D loss: 0.098696, acc: 96.88%] [G loss: 4.158065]\n",
            "******* 2 119 [D loss: 0.033362, acc: 100.00%] [G loss: 4.289240]\n",
            "******* 2 120 [D loss: 0.022468, acc: 100.00%] [G loss: 4.211164]\n",
            "******* 2 121 [D loss: 0.068786, acc: 100.00%] [G loss: 4.489223]\n",
            "******* 2 122 [D loss: 0.006751, acc: 100.00%] [G loss: 5.238777]\n",
            "******* 2 123 [D loss: 0.103725, acc: 96.88%] [G loss: 4.603344]\n",
            "******* 2 124 [D loss: 0.048784, acc: 96.88%] [G loss: 3.648895]\n",
            "******* 2 125 [D loss: 0.088760, acc: 96.88%] [G loss: 3.114005]\n",
            "******* 2 126 [D loss: 0.135624, acc: 96.88%] [G loss: 4.630457]\n",
            "******* 2 127 [D loss: 0.068049, acc: 96.88%] [G loss: 5.899547]\n",
            "******* 2 128 [D loss: 0.114033, acc: 93.75%] [G loss: 6.483804]\n",
            "******* 2 129 [D loss: 0.051733, acc: 100.00%] [G loss: 6.973586]\n",
            "******* 2 130 [D loss: 0.188592, acc: 93.75%] [G loss: 4.273858]\n",
            "******* 2 131 [D loss: 0.079999, acc: 93.75%] [G loss: 4.584720]\n",
            "******* 2 132 [D loss: 0.042447, acc: 96.88%] [G loss: 5.531923]\n",
            "******* 2 133 [D loss: 0.012755, acc: 100.00%] [G loss: 5.196014]\n",
            "******* 2 134 [D loss: 0.015206, acc: 100.00%] [G loss: 6.350623]\n",
            "******* 2 135 [D loss: 0.148274, acc: 96.88%] [G loss: 5.231366]\n",
            "******* 2 136 [D loss: 0.090488, acc: 96.88%] [G loss: 3.129072]\n",
            "******* 2 137 [D loss: 0.355685, acc: 84.38%] [G loss: 3.102917]\n",
            "******* 2 138 [D loss: 0.116796, acc: 96.88%] [G loss: 4.597797]\n",
            "******* 2 139 [D loss: 0.147602, acc: 90.62%] [G loss: 5.797696]\n",
            "******* 2 140 [D loss: 0.529023, acc: 84.38%] [G loss: 6.368647]\n",
            "******* 2 141 [D loss: 0.511949, acc: 87.50%] [G loss: 4.862309]\n",
            "******* 2 142 [D loss: 0.105332, acc: 96.88%] [G loss: 2.905122]\n",
            "******* 2 143 [D loss: 0.169504, acc: 90.62%] [G loss: 4.554966]\n",
            "******* 2 144 [D loss: 0.018157, acc: 100.00%] [G loss: 7.322412]\n",
            "******* 2 145 [D loss: 0.054126, acc: 96.88%] [G loss: 7.653525]\n",
            "******* 2 146 [D loss: 0.193427, acc: 93.75%] [G loss: 6.925460]\n",
            "******* 2 147 [D loss: 0.189324, acc: 96.88%] [G loss: 4.388907]\n",
            "******* 2 148 [D loss: 0.095648, acc: 96.88%] [G loss: 4.009912]\n",
            "******* 2 149 [D loss: 0.198758, acc: 93.75%] [G loss: 4.318744]\n",
            "******* 2 150 [D loss: 0.124603, acc: 93.75%] [G loss: 4.302380]\n",
            "******* 2 151 [D loss: 0.082069, acc: 96.88%] [G loss: 6.015502]\n",
            "******* 2 152 [D loss: 0.041077, acc: 96.88%] [G loss: 7.425346]\n",
            "******* 2 153 [D loss: 0.062209, acc: 96.88%] [G loss: 11.225752]\n",
            "******* 2 154 [D loss: 0.073509, acc: 96.88%] [G loss: 8.658268]\n",
            "******* 2 155 [D loss: 0.436151, acc: 84.38%] [G loss: 6.488346]\n",
            "******* 2 156 [D loss: 0.182678, acc: 90.62%] [G loss: 6.190018]\n",
            "******* 2 157 [D loss: 0.177249, acc: 90.62%] [G loss: 7.332652]\n",
            "******* 2 158 [D loss: 0.142168, acc: 96.88%] [G loss: 12.746513]\n",
            "******* 2 159 [D loss: 0.117933, acc: 96.88%] [G loss: 15.769244]\n",
            "******* 2 160 [D loss: 0.036919, acc: 96.88%] [G loss: 9.996933]\n",
            "******* 2 161 [D loss: 0.050868, acc: 96.88%] [G loss: 14.493507]\n",
            "******* 2 162 [D loss: 0.071911, acc: 96.88%] [G loss: 12.319382]\n",
            "******* 3 0 [D loss: 0.021162, acc: 100.00%] [G loss: 10.558750]\n",
            "******* 3 1 [D loss: 0.099957, acc: 93.75%] [G loss: 7.480914]\n",
            "******* 3 2 [D loss: 0.034174, acc: 100.00%] [G loss: 3.960504]\n",
            "******* 3 3 [D loss: 0.083455, acc: 100.00%] [G loss: 3.625957]\n",
            "******* 3 4 [D loss: 0.505628, acc: 81.25%] [G loss: 6.169860]\n",
            "******* 3 5 [D loss: 0.104707, acc: 96.88%] [G loss: 9.763633]\n",
            "******* 3 6 [D loss: 0.021738, acc: 100.00%] [G loss: 11.400612]\n",
            "******* 3 7 [D loss: 0.092617, acc: 93.75%] [G loss: 15.208902]\n",
            "******* 3 8 [D loss: 0.272281, acc: 90.62%] [G loss: 11.359467]\n",
            "******* 3 9 [D loss: 1.020466, acc: 81.25%] [G loss: 11.962427]\n",
            "******* 3 10 [D loss: 0.072370, acc: 96.88%] [G loss: 15.658459]\n",
            "******* 3 11 [D loss: 0.254481, acc: 93.75%] [G loss: 14.079909]\n",
            "******* 3 12 [D loss: 0.201906, acc: 96.88%] [G loss: 10.753758]\n",
            "******* 3 13 [D loss: 0.298960, acc: 87.50%] [G loss: 7.613569]\n",
            "******* 3 14 [D loss: 0.057117, acc: 96.88%] [G loss: 10.682821]\n",
            "******* 3 15 [D loss: 0.009200, acc: 100.00%] [G loss: 12.485744]\n",
            "******* 3 16 [D loss: 0.141977, acc: 96.88%] [G loss: 10.594603]\n",
            "******* 3 17 [D loss: 0.845228, acc: 81.25%] [G loss: 11.196831]\n",
            "******* 3 18 [D loss: 0.237265, acc: 96.88%] [G loss: 13.684818]\n",
            "******* 3 19 [D loss: 0.709803, acc: 78.12%] [G loss: 6.406758]\n",
            "******* 3 20 [D loss: 0.338928, acc: 84.38%] [G loss: 7.098309]\n",
            "******* 3 21 [D loss: 1.128300, acc: 71.88%] [G loss: 6.361529]\n",
            "******* 3 22 [D loss: 0.947690, acc: 87.50%] [G loss: 6.458136]\n",
            "******* 3 23 [D loss: 1.384343, acc: 75.00%] [G loss: 7.466528]\n",
            "******* 3 24 [D loss: 0.730248, acc: 87.50%] [G loss: 6.215729]\n",
            "******* 3 25 [D loss: 0.246685, acc: 87.50%] [G loss: 5.609968]\n",
            "******* 3 26 [D loss: 0.069806, acc: 96.88%] [G loss: 4.871437]\n",
            "******* 3 27 [D loss: 0.153690, acc: 93.75%] [G loss: 3.934250]\n",
            "******* 3 28 [D loss: 0.262437, acc: 90.62%] [G loss: 3.582203]\n",
            "******* 3 29 [D loss: 0.229192, acc: 87.50%] [G loss: 4.354454]\n",
            "******* 3 30 [D loss: 0.212899, acc: 90.62%] [G loss: 6.615724]\n",
            "******* 3 31 [D loss: 0.805197, acc: 81.25%] [G loss: 3.958389]\n",
            "******* 3 32 [D loss: 0.629518, acc: 75.00%] [G loss: 3.420434]\n",
            "******* 3 33 [D loss: 0.131689, acc: 93.75%] [G loss: 6.583695]\n",
            "******* 3 34 [D loss: 0.130059, acc: 93.75%] [G loss: 9.236313]\n",
            "******* 3 35 [D loss: 0.167682, acc: 87.50%] [G loss: 7.219934]\n",
            "******* 3 36 [D loss: 0.099201, acc: 96.88%] [G loss: 5.756528]\n",
            "******* 3 37 [D loss: 0.213347, acc: 93.75%] [G loss: 4.282855]\n",
            "******* 3 38 [D loss: 0.023539, acc: 100.00%] [G loss: 3.799851]\n",
            "******* 3 39 [D loss: 0.403352, acc: 87.50%] [G loss: 3.520328]\n",
            "******* 3 40 [D loss: 0.018562, acc: 100.00%] [G loss: 7.290666]\n",
            "******* 3 41 [D loss: 0.074237, acc: 96.88%] [G loss: 8.758465]\n",
            "******* 3 42 [D loss: 0.205281, acc: 87.50%] [G loss: 7.310399]\n",
            "******* 3 43 [D loss: 0.051589, acc: 100.00%] [G loss: 6.162120]\n",
            "******* 3 44 [D loss: 0.022971, acc: 100.00%] [G loss: 5.108105]\n",
            "******* 3 45 [D loss: 0.032126, acc: 100.00%] [G loss: 5.520297]\n",
            "******* 3 46 [D loss: 0.025059, acc: 100.00%] [G loss: 4.843440]\n",
            "******* 3 47 [D loss: 0.059754, acc: 96.88%] [G loss: 4.984930]\n",
            "******* 3 48 [D loss: 0.046021, acc: 100.00%] [G loss: 5.101143]\n",
            "******* 3 49 [D loss: 0.060858, acc: 100.00%] [G loss: 5.645853]\n",
            "******* 3 50 [D loss: 0.022480, acc: 100.00%] [G loss: 4.735563]\n",
            "******* 3 51 [D loss: 0.026207, acc: 100.00%] [G loss: 6.819826]\n",
            "******* 3 52 [D loss: 0.014045, acc: 100.00%] [G loss: 7.143743]\n",
            "******* 3 53 [D loss: 0.026254, acc: 100.00%] [G loss: 7.463855]\n",
            "******* 3 54 [D loss: 0.010467, acc: 100.00%] [G loss: 7.973415]\n",
            "******* 3 55 [D loss: 0.008944, acc: 100.00%] [G loss: 8.054733]\n",
            "******* 3 56 [D loss: 0.006535, acc: 100.00%] [G loss: 8.241134]\n",
            "******* 3 57 [D loss: 0.028040, acc: 100.00%] [G loss: 6.718357]\n",
            "******* 3 58 [D loss: 0.019921, acc: 100.00%] [G loss: 6.140599]\n",
            "******* 3 59 [D loss: 0.036910, acc: 96.88%] [G loss: 4.187377]\n",
            "******* 3 60 [D loss: 0.026366, acc: 100.00%] [G loss: 4.878381]\n",
            "******* 3 61 [D loss: 0.034202, acc: 100.00%] [G loss: 4.056996]\n",
            "******* 3 62 [D loss: 0.018515, acc: 100.00%] [G loss: 4.751105]\n",
            "******* 3 63 [D loss: 0.010123, acc: 100.00%] [G loss: 4.082438]\n",
            "******* 3 64 [D loss: 0.011158, acc: 100.00%] [G loss: 5.418923]\n",
            "******* 3 65 [D loss: 0.006547, acc: 100.00%] [G loss: 5.699170]\n",
            "******* 3 66 [D loss: 0.130970, acc: 93.75%] [G loss: 4.573096]\n",
            "******* 3 67 [D loss: 0.030608, acc: 100.00%] [G loss: 5.067016]\n",
            "******* 3 68 [D loss: 0.059286, acc: 96.88%] [G loss: 4.753316]\n",
            "******* 3 69 [D loss: 0.004461, acc: 100.00%] [G loss: 5.808416]\n",
            "******* 3 70 [D loss: 0.028247, acc: 96.88%] [G loss: 6.817256]\n",
            "******* 3 71 [D loss: 0.069935, acc: 96.88%] [G loss: 6.801508]\n",
            "******* 3 72 [D loss: 0.010032, acc: 100.00%] [G loss: 7.382209]\n",
            "******* 3 73 [D loss: 0.017140, acc: 100.00%] [G loss: 7.614893]\n",
            "******* 3 74 [D loss: 0.046254, acc: 96.88%] [G loss: 7.534492]\n",
            "******* 3 75 [D loss: 0.020900, acc: 100.00%] [G loss: 7.726565]\n",
            "******* 3 76 [D loss: 0.022181, acc: 100.00%] [G loss: 7.889239]\n",
            "******* 3 77 [D loss: 0.012650, acc: 100.00%] [G loss: 7.835764]\n",
            "******* 3 78 [D loss: 0.010006, acc: 100.00%] [G loss: 7.861522]\n",
            "******* 3 79 [D loss: 0.003095, acc: 100.00%] [G loss: 6.208927]\n",
            "******* 3 80 [D loss: 0.107590, acc: 96.88%] [G loss: 7.458348]\n",
            "******* 3 81 [D loss: 0.089220, acc: 96.88%] [G loss: 5.727717]\n",
            "******* 3 82 [D loss: 0.015756, acc: 100.00%] [G loss: 5.829154]\n",
            "******* 3 83 [D loss: 0.020114, acc: 100.00%] [G loss: 5.542591]\n",
            "******* 3 84 [D loss: 0.009834, acc: 100.00%] [G loss: 5.168156]\n",
            "******* 3 85 [D loss: 0.013357, acc: 100.00%] [G loss: 5.020371]\n",
            "******* 3 86 [D loss: 0.046167, acc: 96.88%] [G loss: 5.264412]\n",
            "******* 3 87 [D loss: 0.013896, acc: 100.00%] [G loss: 4.999541]\n",
            "******* 3 88 [D loss: 0.047317, acc: 100.00%] [G loss: 4.702406]\n",
            "******* 3 89 [D loss: 0.051769, acc: 96.88%] [G loss: 5.129804]\n",
            "******* 3 90 [D loss: 0.009124, acc: 100.00%] [G loss: 4.733818]\n",
            "******* 3 91 [D loss: 0.023344, acc: 100.00%] [G loss: 4.345421]\n",
            "******* 3 92 [D loss: 0.007832, acc: 100.00%] [G loss: 6.654348]\n",
            "******* 3 93 [D loss: 0.007668, acc: 100.00%] [G loss: 7.194627]\n",
            "******* 3 94 [D loss: 0.002665, acc: 100.00%] [G loss: 7.137218]\n",
            "******* 3 95 [D loss: 0.005132, acc: 100.00%] [G loss: 7.674746]\n",
            "******* 3 96 [D loss: 0.007329, acc: 100.00%] [G loss: 6.206484]\n",
            "******* 3 97 [D loss: 0.019267, acc: 100.00%] [G loss: 8.831017]\n",
            "******* 3 98 [D loss: 0.038515, acc: 96.88%] [G loss: 7.018465]\n",
            "******* 3 99 [D loss: 0.075006, acc: 96.88%] [G loss: 5.962122]\n",
            "******* 3 100 [D loss: 0.066701, acc: 96.88%] [G loss: 5.587199]\n",
            "******* 3 101 [D loss: 0.083686, acc: 93.75%] [G loss: 6.146701]\n",
            "******* 3 102 [D loss: 0.656728, acc: 78.12%] [G loss: 6.145893]\n",
            "******* 3 103 [D loss: 0.085323, acc: 96.88%] [G loss: 8.771336]\n",
            "******* 3 104 [D loss: 0.155346, acc: 96.88%] [G loss: 14.507570]\n",
            "******* 3 105 [D loss: 0.091507, acc: 93.75%] [G loss: 15.649681]\n",
            "******* 3 106 [D loss: 0.684248, acc: 87.50%] [G loss: 11.615675]\n",
            "******* 3 107 [D loss: 0.381535, acc: 81.25%] [G loss: 5.317191]\n",
            "******* 3 108 [D loss: 0.646142, acc: 78.12%] [G loss: 7.633528]\n",
            "******* 3 109 [D loss: 0.163608, acc: 93.75%] [G loss: 16.180637]\n",
            "******* 3 110 [D loss: 0.287776, acc: 81.25%] [G loss: 12.325537]\n",
            "******* 3 111 [D loss: 0.075698, acc: 96.88%] [G loss: 10.165401]\n",
            "******* 3 112 [D loss: 0.042845, acc: 96.88%] [G loss: 8.280872]\n",
            "******* 3 113 [D loss: 0.156192, acc: 93.75%] [G loss: 10.051541]\n",
            "******* 3 114 [D loss: 0.167916, acc: 93.75%] [G loss: 6.607957]\n",
            "******* 3 115 [D loss: 0.785468, acc: 84.38%] [G loss: 9.278211]\n",
            "******* 3 116 [D loss: 0.099823, acc: 93.75%] [G loss: 15.405993]\n",
            "******* 3 117 [D loss: 0.357469, acc: 90.62%] [G loss: 20.276382]\n",
            "******* 3 118 [D loss: 0.176463, acc: 90.62%] [G loss: 15.886761]\n",
            "******* 3 119 [D loss: 0.461407, acc: 90.62%] [G loss: 6.934072]\n",
            "******* 3 120 [D loss: 0.890307, acc: 71.88%] [G loss: 12.336077]\n",
            "******* 3 121 [D loss: 0.233898, acc: 96.88%] [G loss: 27.243847]\n",
            "******* 3 122 [D loss: 0.056067, acc: 96.88%] [G loss: 34.366913]\n",
            "******* 3 123 [D loss: 0.222065, acc: 93.75%] [G loss: 27.625774]\n",
            "******* 3 124 [D loss: 0.197208, acc: 90.62%] [G loss: 18.568981]\n",
            "******* 3 125 [D loss: 4.359428, acc: 53.12%] [G loss: 24.391285]\n",
            "******* 3 126 [D loss: 0.932398, acc: 81.25%] [G loss: 37.814243]\n",
            "******* 3 127 [D loss: 0.003357, acc: 100.00%] [G loss: 48.481487]\n",
            "******* 3 128 [D loss: 2.776284, acc: 81.25%] [G loss: 12.820654]\n",
            "******* 3 129 [D loss: 9.284666, acc: 46.88%] [G loss: 74.572052]\n",
            "******* 3 130 [D loss: 2.614122, acc: 84.38%] [G loss: 129.489014]\n",
            "******* 3 131 [D loss: 8.005262, acc: 75.00%] [G loss: 122.818848]\n",
            "******* 3 132 [D loss: 9.407646, acc: 78.12%] [G loss: 73.189629]\n",
            "******* 3 133 [D loss: 1.626341, acc: 90.62%] [G loss: 33.075142]\n",
            "******* 3 134 [D loss: 0.040181, acc: 96.88%] [G loss: 3.671808]\n",
            "******* 3 135 [D loss: 3.795877, acc: 50.00%] [G loss: 8.473879]\n",
            "******* 3 136 [D loss: 0.000604, acc: 100.00%] [G loss: 32.636917]\n",
            "******* 3 137 [D loss: 0.269708, acc: 93.75%] [G loss: 47.376568]\n",
            "******* 3 138 [D loss: 1.023716, acc: 90.62%] [G loss: 49.892517]\n",
            "******* 3 139 [D loss: 1.372411, acc: 90.62%] [G loss: 44.307014]\n",
            "******* 3 140 [D loss: 1.260319, acc: 93.75%] [G loss: 32.795509]\n",
            "******* 3 141 [D loss: 0.000000, acc: 100.00%] [G loss: 23.241665]\n",
            "******* 3 142 [D loss: 0.000000, acc: 100.00%] [G loss: 12.770966]\n",
            "******* 3 143 [D loss: 0.003116, acc: 100.00%] [G loss: 5.661519]\n",
            "******* 3 144 [D loss: 0.561453, acc: 93.75%] [G loss: 9.118580]\n",
            "******* 3 145 [D loss: 0.000003, acc: 100.00%] [G loss: 21.936747]\n",
            "******* 3 146 [D loss: 0.000005, acc: 100.00%] [G loss: 26.287598]\n",
            "******* 3 147 [D loss: 0.000057, acc: 100.00%] [G loss: 30.292530]\n",
            "******* 3 148 [D loss: 0.000000, acc: 100.00%] [G loss: 31.921537]\n",
            "******* 3 149 [D loss: 0.650270, acc: 93.75%] [G loss: 31.782627]\n",
            "******* 3 150 [D loss: 3.105113, acc: 87.50%] [G loss: 30.219511]\n",
            "******* 3 151 [D loss: 0.705603, acc: 93.75%] [G loss: 25.846640]\n",
            "******* 3 152 [D loss: 3.345781, acc: 90.62%] [G loss: 26.701412]\n",
            "******* 3 153 [D loss: 0.221773, acc: 96.88%] [G loss: 21.194099]\n",
            "******* 3 154 [D loss: 0.680456, acc: 90.62%] [G loss: 23.354301]\n",
            "******* 3 155 [D loss: 0.000113, acc: 100.00%] [G loss: 20.080704]\n",
            "******* 3 156 [D loss: 1.558258, acc: 81.25%] [G loss: 20.017216]\n",
            "******* 3 157 [D loss: 0.432160, acc: 93.75%] [G loss: 15.861271]\n",
            "******* 3 158 [D loss: 0.072794, acc: 96.88%] [G loss: 12.200403]\n",
            "******* 3 159 [D loss: 0.371415, acc: 96.88%] [G loss: 16.087292]\n",
            "******* 3 160 [D loss: 0.642046, acc: 93.75%] [G loss: 13.166293]\n",
            "******* 3 161 [D loss: 0.196229, acc: 96.88%] [G loss: 15.472618]\n",
            "******* 3 162 [D loss: 0.005718, acc: 100.00%] [G loss: 22.214649]\n",
            "******* 4 0 [D loss: 0.738130, acc: 87.50%] [G loss: 17.940586]\n",
            "******* 4 1 [D loss: 0.589021, acc: 96.88%] [G loss: 15.151405]\n",
            "******* 4 2 [D loss: 0.537584, acc: 84.38%] [G loss: 14.100752]\n",
            "******* 4 3 [D loss: 0.354361, acc: 96.88%] [G loss: 14.977614]\n",
            "******* 4 4 [D loss: 1.540461, acc: 84.38%] [G loss: 12.418542]\n",
            "******* 4 5 [D loss: 0.173143, acc: 96.88%] [G loss: 10.299858]\n",
            "******* 4 6 [D loss: 0.406014, acc: 93.75%] [G loss: 7.647578]\n",
            "******* 4 7 [D loss: 0.066923, acc: 96.88%] [G loss: 8.108778]\n",
            "******* 4 8 [D loss: 0.011057, acc: 100.00%] [G loss: 9.139912]\n",
            "******* 4 9 [D loss: 0.007731, acc: 100.00%] [G loss: 9.805347]\n",
            "******* 4 10 [D loss: 0.002939, acc: 100.00%] [G loss: 10.199091]\n",
            "******* 4 11 [D loss: 0.093188, acc: 93.75%] [G loss: 10.408064]\n",
            "******* 4 12 [D loss: 0.092428, acc: 93.75%] [G loss: 7.612132]\n",
            "******* 4 13 [D loss: 0.103649, acc: 93.75%] [G loss: 8.350520]\n",
            "******* 4 14 [D loss: 0.093718, acc: 93.75%] [G loss: 7.117061]\n",
            "******* 4 15 [D loss: 0.005019, acc: 100.00%] [G loss: 9.919628]\n",
            "******* 4 16 [D loss: 0.044645, acc: 96.88%] [G loss: 11.420356]\n",
            "******* 4 17 [D loss: 0.000057, acc: 100.00%] [G loss: 10.580949]\n",
            "******* 4 18 [D loss: 0.281713, acc: 96.88%] [G loss: 8.331573]\n",
            "******* 4 19 [D loss: 0.001911, acc: 100.00%] [G loss: 9.215488]\n",
            "******* 4 20 [D loss: 0.023046, acc: 100.00%] [G loss: 7.733613]\n",
            "******* 4 21 [D loss: 0.004566, acc: 100.00%] [G loss: 7.766626]\n",
            "******* 4 22 [D loss: 0.004580, acc: 100.00%] [G loss: 7.947580]\n",
            "******* 4 23 [D loss: 0.004968, acc: 100.00%] [G loss: 6.510173]\n",
            "******* 4 24 [D loss: 0.066636, acc: 96.88%] [G loss: 7.543389]\n",
            "******* 4 25 [D loss: 0.112582, acc: 96.88%] [G loss: 5.517806]\n",
            "******* 4 26 [D loss: 0.011175, acc: 100.00%] [G loss: 6.952373]\n",
            "******* 4 27 [D loss: 0.001971, acc: 100.00%] [G loss: 7.374699]\n",
            "******* 4 28 [D loss: 0.001403, acc: 100.00%] [G loss: 7.956125]\n",
            "******* 4 29 [D loss: 0.006926, acc: 100.00%] [G loss: 8.216133]\n",
            "******* 4 30 [D loss: 0.003917, acc: 100.00%] [G loss: 7.764686]\n",
            "******* 4 31 [D loss: 0.160970, acc: 96.88%] [G loss: 9.896717]\n",
            "******* 4 32 [D loss: 0.043766, acc: 96.88%] [G loss: 7.718043]\n",
            "******* 4 33 [D loss: 0.001603, acc: 100.00%] [G loss: 7.028250]\n",
            "******* 4 34 [D loss: 0.009238, acc: 100.00%] [G loss: 5.884314]\n",
            "******* 4 35 [D loss: 0.030781, acc: 100.00%] [G loss: 4.983208]\n",
            "******* 4 36 [D loss: 0.006558, acc: 100.00%] [G loss: 6.442917]\n",
            "******* 4 37 [D loss: 0.006412, acc: 100.00%] [G loss: 8.142786]\n",
            "******* 4 38 [D loss: 0.159936, acc: 96.88%] [G loss: 7.255097]\n",
            "******* 4 39 [D loss: 0.002480, acc: 100.00%] [G loss: 8.024231]\n",
            "******* 4 40 [D loss: 0.002567, acc: 100.00%] [G loss: 7.385012]\n",
            "******* 4 41 [D loss: 0.005102, acc: 100.00%] [G loss: 7.642058]\n",
            "******* 4 42 [D loss: 0.011479, acc: 100.00%] [G loss: 7.564462]\n",
            "******* 4 43 [D loss: 0.002435, acc: 100.00%] [G loss: 8.854530]\n",
            "******* 4 44 [D loss: 0.000235, acc: 100.00%] [G loss: 8.736209]\n",
            "******* 4 45 [D loss: 0.024995, acc: 100.00%] [G loss: 8.793360]\n",
            "******* 4 46 [D loss: 0.000546, acc: 100.00%] [G loss: 9.976101]\n",
            "******* 4 47 [D loss: 0.007439, acc: 100.00%] [G loss: 9.033930]\n",
            "******* 4 48 [D loss: 0.004659, acc: 100.00%] [G loss: 8.900617]\n",
            "******* 4 49 [D loss: 0.001627, acc: 100.00%] [G loss: 10.762119]\n",
            "******* 4 50 [D loss: 0.000429, acc: 100.00%] [G loss: 9.083774]\n",
            "******* 4 51 [D loss: 0.000345, acc: 100.00%] [G loss: 11.460089]\n",
            "******* 4 52 [D loss: 0.002264, acc: 100.00%] [G loss: 13.225470]\n",
            "******* 4 53 [D loss: 0.000143, acc: 100.00%] [G loss: 9.085402]\n",
            "******* 4 54 [D loss: 0.001904, acc: 100.00%] [G loss: 10.449287]\n",
            "******* 4 55 [D loss: 0.053425, acc: 96.88%] [G loss: 9.380044]\n",
            "******* 4 56 [D loss: 0.001624, acc: 100.00%] [G loss: 9.742995]\n",
            "******* 4 57 [D loss: 0.009219, acc: 100.00%] [G loss: 7.875548]\n",
            "******* 4 58 [D loss: 0.008746, acc: 100.00%] [G loss: 7.809581]\n",
            "******* 4 59 [D loss: 0.011096, acc: 100.00%] [G loss: 7.851439]\n",
            "******* 4 60 [D loss: 0.094704, acc: 96.88%] [G loss: 8.421600]\n",
            "******* 4 61 [D loss: 0.007235, acc: 100.00%] [G loss: 10.923540]\n",
            "******* 4 62 [D loss: 0.015052, acc: 100.00%] [G loss: 12.350777]\n",
            "******* 4 63 [D loss: 0.000034, acc: 100.00%] [G loss: 14.440294]\n",
            "******* 4 64 [D loss: 0.000575, acc: 100.00%] [G loss: 13.614594]\n",
            "******* 4 65 [D loss: 0.186292, acc: 93.75%] [G loss: 10.886509]\n",
            "******* 4 66 [D loss: 0.001251, acc: 100.00%] [G loss: 9.740733]\n",
            "******* 4 67 [D loss: 0.014716, acc: 100.00%] [G loss: 5.966251]\n",
            "******* 4 68 [D loss: 0.064871, acc: 93.75%] [G loss: 7.788596]\n",
            "******* 4 69 [D loss: 0.001253, acc: 100.00%] [G loss: 10.116241]\n",
            "******* 4 70 [D loss: 0.001976, acc: 100.00%] [G loss: 9.001724]\n",
            "******* 4 71 [D loss: 0.000311, acc: 100.00%] [G loss: 11.212096]\n",
            "******* 4 72 [D loss: 0.000350, acc: 100.00%] [G loss: 12.274141]\n",
            "******* 4 73 [D loss: 0.254664, acc: 96.88%] [G loss: 13.251276]\n",
            "******* 4 74 [D loss: 0.003520, acc: 100.00%] [G loss: 11.454411]\n",
            "******* 4 75 [D loss: 0.112356, acc: 93.75%] [G loss: 11.921438]\n",
            "******* 4 76 [D loss: 0.193386, acc: 93.75%] [G loss: 10.634175]\n",
            "******* 4 77 [D loss: 0.040233, acc: 96.88%] [G loss: 14.368710]\n",
            "******* 4 78 [D loss: 0.297244, acc: 87.50%] [G loss: 14.011070]\n",
            "******* 4 79 [D loss: 0.093661, acc: 93.75%] [G loss: 9.421391]\n",
            "******* 4 80 [D loss: 0.268770, acc: 96.88%] [G loss: 10.138588]\n",
            "******* 4 81 [D loss: 0.246759, acc: 90.62%] [G loss: 9.396180]\n",
            "******* 4 82 [D loss: 0.000574, acc: 100.00%] [G loss: 12.065475]\n",
            "******* 4 83 [D loss: 0.019800, acc: 100.00%] [G loss: 12.568714]\n",
            "******* 4 84 [D loss: 0.009837, acc: 100.00%] [G loss: 13.737759]\n",
            "******* 4 85 [D loss: 0.000506, acc: 100.00%] [G loss: 13.520568]\n",
            "******* 4 86 [D loss: 0.011331, acc: 100.00%] [G loss: 12.168190]\n",
            "******* 4 87 [D loss: 0.000470, acc: 100.00%] [G loss: 14.835806]\n",
            "******* 4 88 [D loss: 0.008295, acc: 100.00%] [G loss: 14.829090]\n",
            "******* 4 89 [D loss: 0.005012, acc: 100.00%] [G loss: 13.459702]\n",
            "******* 4 90 [D loss: 0.022272, acc: 100.00%] [G loss: 12.703251]\n",
            "******* 4 91 [D loss: 0.000208, acc: 100.00%] [G loss: 12.532887]\n",
            "******* 4 92 [D loss: 0.018225, acc: 100.00%] [G loss: 13.067400]\n",
            "******* 4 93 [D loss: 0.000042, acc: 100.00%] [G loss: 11.864328]\n",
            "******* 4 94 [D loss: 0.222312, acc: 96.88%] [G loss: 11.223544]\n",
            "******* 4 95 [D loss: 0.416299, acc: 93.75%] [G loss: 15.373980]\n",
            "******* 4 96 [D loss: 0.000281, acc: 100.00%] [G loss: 22.868952]\n",
            "******* 4 97 [D loss: 1.223053, acc: 87.50%] [G loss: 20.868294]\n",
            "******* 4 98 [D loss: 0.366949, acc: 93.75%] [G loss: 20.986996]\n",
            "******* 4 99 [D loss: 0.003218, acc: 100.00%] [G loss: 19.045673]\n",
            "******* 4 100 [D loss: 0.001904, acc: 100.00%] [G loss: 15.677879]\n",
            "******* 4 101 [D loss: 0.005537, acc: 100.00%] [G loss: 14.028628]\n",
            "******* 4 102 [D loss: 0.000144, acc: 100.00%] [G loss: 11.125340]\n",
            "******* 4 103 [D loss: 0.024462, acc: 96.88%] [G loss: 10.551979]\n",
            "******* 4 104 [D loss: 0.489054, acc: 96.88%] [G loss: 8.655903]\n",
            "******* 4 105 [D loss: 0.463902, acc: 84.38%] [G loss: 21.504944]\n",
            "******* 4 106 [D loss: 0.165686, acc: 93.75%] [G loss: 34.359932]\n",
            "******* 4 107 [D loss: 0.583347, acc: 90.62%] [G loss: 37.213398]\n",
            "******* 4 108 [D loss: 1.834526, acc: 87.50%] [G loss: 25.977560]\n",
            "******* 4 109 [D loss: 0.319187, acc: 96.88%] [G loss: 17.899349]\n",
            "******* 4 110 [D loss: 2.105403, acc: 81.25%] [G loss: 23.673733]\n",
            "******* 4 111 [D loss: 0.421296, acc: 93.75%] [G loss: 44.237877]\n",
            "******* 4 112 [D loss: 1.424933, acc: 90.62%] [G loss: 43.068993]\n",
            "******* 4 113 [D loss: 1.029031, acc: 96.88%] [G loss: 42.040154]\n",
            "******* 4 114 [D loss: 1.079304, acc: 90.62%] [G loss: 37.838928]\n",
            "******* 4 115 [D loss: 1.760643, acc: 90.62%] [G loss: 26.951443]\n",
            "******* 4 116 [D loss: 1.457025, acc: 87.50%] [G loss: 16.049179]\n",
            "******* 4 117 [D loss: 0.405348, acc: 90.62%] [G loss: 4.000755]\n",
            "******* 4 118 [D loss: 1.148595, acc: 71.88%] [G loss: 6.692019]\n",
            "******* 4 119 [D loss: 0.131684, acc: 90.62%] [G loss: 10.192595]\n",
            "******* 4 120 [D loss: 0.643034, acc: 90.62%] [G loss: 9.235378]\n",
            "******* 4 121 [D loss: 0.014267, acc: 100.00%] [G loss: 12.109722]\n",
            "******* 4 122 [D loss: 0.025473, acc: 100.00%] [G loss: 11.648936]\n",
            "******* 4 123 [D loss: 0.219014, acc: 96.88%] [G loss: 10.063873]\n",
            "******* 4 124 [D loss: 0.014724, acc: 100.00%] [G loss: 9.843863]\n",
            "******* 4 125 [D loss: 0.292374, acc: 96.88%] [G loss: 8.179064]\n",
            "******* 4 126 [D loss: 0.403973, acc: 93.75%] [G loss: 5.275904]\n",
            "******* 4 127 [D loss: 0.020298, acc: 100.00%] [G loss: 3.676374]\n",
            "******* 4 128 [D loss: 0.076016, acc: 100.00%] [G loss: 2.201354]\n",
            "******* 4 129 [D loss: 0.127372, acc: 93.75%] [G loss: 3.320308]\n",
            "******* 4 130 [D loss: 0.031290, acc: 100.00%] [G loss: 4.481551]\n",
            "******* 4 131 [D loss: 0.030680, acc: 100.00%] [G loss: 5.388016]\n",
            "******* 4 132 [D loss: 0.111032, acc: 96.88%] [G loss: 7.153370]\n",
            "******* 4 133 [D loss: 0.099536, acc: 96.88%] [G loss: 7.322546]\n",
            "******* 4 134 [D loss: 0.097332, acc: 96.88%] [G loss: 7.234264]\n",
            "******* 4 135 [D loss: 0.001645, acc: 100.00%] [G loss: 6.885050]\n",
            "******* 4 136 [D loss: 0.001867, acc: 100.00%] [G loss: 6.295355]\n",
            "******* 4 137 [D loss: 0.155112, acc: 93.75%] [G loss: 5.106812]\n",
            "******* 4 138 [D loss: 0.181490, acc: 93.75%] [G loss: 5.490170]\n",
            "******* 4 139 [D loss: 0.060221, acc: 96.88%] [G loss: 4.650132]\n",
            "******* 4 140 [D loss: 0.037478, acc: 96.88%] [G loss: 4.794923]\n",
            "******* 4 141 [D loss: 0.023477, acc: 100.00%] [G loss: 5.578611]\n",
            "******* 4 142 [D loss: 0.165958, acc: 96.88%] [G loss: 5.139092]\n",
            "******* 4 143 [D loss: 0.004175, acc: 100.00%] [G loss: 4.824640]\n",
            "******* 4 144 [D loss: 0.026750, acc: 100.00%] [G loss: 4.903559]\n",
            "******* 4 145 [D loss: 0.430999, acc: 90.62%] [G loss: 3.539575]\n",
            "******* 4 146 [D loss: 0.030994, acc: 100.00%] [G loss: 2.805101]\n",
            "******* 4 147 [D loss: 0.107538, acc: 96.88%] [G loss: 2.450046]\n",
            "******* 4 148 [D loss: 0.041054, acc: 100.00%] [G loss: 2.876094]\n",
            "******* 4 149 [D loss: 0.020628, acc: 100.00%] [G loss: 4.044477]\n",
            "******* 4 150 [D loss: 0.073700, acc: 96.88%] [G loss: 4.457690]\n",
            "******* 4 151 [D loss: 0.049798, acc: 96.88%] [G loss: 4.896968]\n",
            "******* 4 152 [D loss: 0.003908, acc: 100.00%] [G loss: 5.211565]\n",
            "******* 4 153 [D loss: 0.115156, acc: 96.88%] [G loss: 5.250816]\n",
            "******* 4 154 [D loss: 0.003709, acc: 100.00%] [G loss: 5.433443]\n",
            "******* 4 155 [D loss: 0.001832, acc: 100.00%] [G loss: 5.351482]\n",
            "******* 4 156 [D loss: 0.003317, acc: 100.00%] [G loss: 5.630264]\n",
            "******* 4 157 [D loss: 0.009580, acc: 100.00%] [G loss: 5.705402]\n",
            "******* 4 158 [D loss: 0.007812, acc: 100.00%] [G loss: 5.744917]\n",
            "******* 4 159 [D loss: 0.012484, acc: 100.00%] [G loss: 5.654128]\n",
            "******* 4 160 [D loss: 0.003412, acc: 100.00%] [G loss: 5.446815]\n",
            "******* 4 161 [D loss: 0.005038, acc: 100.00%] [G loss: 5.185272]\n",
            "******* 4 162 [D loss: 0.004102, acc: 100.00%] [G loss: 5.507307]\n",
            "******* 5 0 [D loss: 0.137208, acc: 96.88%] [G loss: 4.810998]\n",
            "******* 5 1 [D loss: 0.115298, acc: 96.88%] [G loss: 4.010197]\n",
            "******* 5 2 [D loss: 0.023075, acc: 100.00%] [G loss: 3.443434]\n",
            "******* 5 3 [D loss: 0.017250, acc: 100.00%] [G loss: 3.840891]\n",
            "******* 5 4 [D loss: 0.026106, acc: 100.00%] [G loss: 4.126386]\n",
            "******* 5 5 [D loss: 0.007401, acc: 100.00%] [G loss: 5.083443]\n",
            "******* 5 6 [D loss: 0.005304, acc: 100.00%] [G loss: 5.470139]\n",
            "******* 5 7 [D loss: 0.002293, acc: 100.00%] [G loss: 5.879577]\n",
            "******* 5 8 [D loss: 0.018171, acc: 100.00%] [G loss: 6.645964]\n",
            "******* 5 9 [D loss: 0.002240, acc: 100.00%] [G loss: 7.020162]\n",
            "******* 5 10 [D loss: 0.004541, acc: 100.00%] [G loss: 7.095772]\n",
            "******* 5 11 [D loss: 0.001631, acc: 100.00%] [G loss: 7.006042]\n",
            "******* 5 12 [D loss: 0.003178, acc: 100.00%] [G loss: 7.091237]\n",
            "******* 5 13 [D loss: 0.000731, acc: 100.00%] [G loss: 7.083108]\n",
            "******* 5 14 [D loss: 0.006522, acc: 100.00%] [G loss: 6.929158]\n",
            "******* 5 15 [D loss: 0.240173, acc: 93.75%] [G loss: 5.977135]\n",
            "******* 5 16 [D loss: 0.004604, acc: 100.00%] [G loss: 5.181701]\n",
            "******* 5 17 [D loss: 0.005527, acc: 100.00%] [G loss: 4.804594]\n",
            "******* 5 18 [D loss: 0.078545, acc: 96.88%] [G loss: 3.963840]\n",
            "******* 5 19 [D loss: 0.018730, acc: 100.00%] [G loss: 3.574721]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT7_Wk-TS_n2"
      },
      "source": [
        "noise = np.random.normal(0, 1, (16, latent_dim))\n",
        "gen_imgs = generator.predict(noise)\n",
        "gen_imgs = (gen_imgs + 1) / 2.0\n",
        "# plt.imshow(gen_imgs[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbWAZ1v_TdJd"
      },
      "source": [
        "plt.imshow(gen_imgs[6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8YtpQpkRvRI"
      },
      "source": [
        "generator.save_weights(\"/content/drive/MyDrive/models/generator1hour.h5\")\n",
        "discriminator.save_weights(\"/content/drive/MyDrive/models/discriminator1hour.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-jSQoN1Azl"
      },
      "source": [
        "### **8) Making GIF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPShgQpg1EMy"
      },
      "source": [
        "# Display a single image using the epoch number\n",
        "# def display_image(epoch_no):\n",
        "#   return PIL.Image.open('generated_images/%.8f.png'.format(epoch_no))\n",
        "\n",
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('generated_images/*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogrmQ73ZR_Wi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}